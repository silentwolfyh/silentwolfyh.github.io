---
title: 2022.4.19-Nature子刊
date: 2022-10-25 10:00
author: NGNLab
toc: false
---

恭喜实验室博士生武楚涵等同学录用Nature Communications文章
[1] Wu, C., Wu, F., Lyu, L. et al. Communication-efficient federated learning via knowledge distillation. Nat Commun 13, 2032 (2022). https://doi.org/10.1038/s41467-022-29763-x

本文提出了一种基于知识蒸馏的去中心化学习框架，降低联邦学习中的通信开销。能够以不到十分之一的通信开销，实现与中心化训练大模型相近的性能。

